{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "\n",
    "from pathlib import Path\n",
    "from EvEye.model.DavisEyeEllipse.EPNet.EPNet import EPNet\n",
    "from EvEye.utils.tonic.functional.ToFrameStack import to_frame_stack_numpy\n",
    "from EvEye.utils.cache.MemmapCacheStructedEvents import *\n",
    "from EvEye.utils.visualization.visualization import *\n",
    "from EvEye.utils.tonic.functional.CutMaxCount import cut_max_count\n",
    "from EvEye.dataset.DavisEyeEllipse.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100\n",
    "\n",
    "data_path = Path(\"/mnt/data2T/junyuan/Datasets/FixedTime10000Dataset/train/cached_data\")\n",
    "ellipse_path = Path(\n",
    "    \"/mnt/data2T/junyuan/Datasets/FixedTime10000Dataset/train/cached_ellipse\"\n",
    ")\n",
    "model_path = Path(\n",
    "    \"/mnt/data2T/junyuan/eye-tracking/logs/EPNet_FixedTime10000/version_0/checkpoints/epochepoch=51-val_lossval_loss=12.4443.ckpt\"\n",
    ")\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "ellipse = convert_to_ellipse(load_ellipse(index, ellipse_path))\n",
    "event_segment = load_event_segment(index, data_path, 5000)\n",
    "model = EPNet(input_channels=2)\n",
    "model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_to_frame(\n",
    "    event_segment,\n",
    "    sensor_size=(346, 260, 2),\n",
    "    events_interpolation='causal_linear',\n",
    "    weight=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a segment of events to a frame stack.\n",
    "    HWC, shape: (260, 346, 2).\n",
    "\n",
    "    Args:\n",
    "        event_segment (np.array): structed array of events with fields ['x', 'y', 'p', 't']\n",
    "        sensor_size (tuple): size of the sensor (width, height, n_channels)\n",
    "        events_interpolation (str): interpolation mode for the events\n",
    "        weight (int): weight for the events\n",
    "\n",
    "    Returns:\n",
    "        event_frame (np.array): frame stack of events. Shape (height, width, n_channels)\n",
    "\n",
    "    \"\"\"\n",
    "    event_frame = to_frame_stack_numpy(\n",
    "        events=event_segment,\n",
    "        sensor_size=sensor_size,\n",
    "        n_time_bins=1,\n",
    "        mode=events_interpolation,\n",
    "        start_time=event_segment['t'][0],\n",
    "        end_time=event_segment['t'][-1],\n",
    "        weight=weight,\n",
    "    ).squeeze(0)\n",
    "    cut_max_count(event_frame, 255)\n",
    "    event_frame = np.moveaxis(event_frame, 0, -1)\n",
    "\n",
    "    return event_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_frame = event_to_frame(event_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(event_frame):\n",
    "    # event_frame\n",
    "    # HWC, shape: (260, 346, 2) -> (256, 256, 2)\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    augment = transform(image=event_frame)\n",
    "    event_frame = augment[\"image\"]\n",
    "    event_frame = (event_frame / 255.0).astype(np.float32)\n",
    "    # shape: (256, 256, 2) -> (2, 256, 256)\n",
    "    event_frame = np.moveaxis(event_frame, -1, 0)\n",
    "    # shape: (2, 256, 256) -> (1, 2, 256, 256)\n",
    "    event_frame = np.expand_dims(event_frame, axis=0)\n",
    "    # type: np.array -> torch.tensor\n",
    "    event_frame = torch.from_numpy(event_frame)\n",
    "\n",
    "    return event_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = pre_process(event_frame)\n",
    "input = input.to(device)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2cpu(cuda_dict):\n",
    "    cpu_dict = {}\n",
    "    for key, value in cuda_dict.items():\n",
    "        cpu_dict[key] = value.cpu()\n",
    "    return cpu_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = dict2cpu(pred)\n",
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredDecoder:\n",
    "    def __init__(self, pred):\n",
    "        self.pred = pred\n",
    "        self.hm = pred[\"hm\"].sigmoid_()\n",
    "        self.ang = pred[\"ang\"]\n",
    "        self.ab = pred[\"ab\"]\n",
    "        self.reg = pred[\"reg\"]\n",
    "\n",
    "    def transpose_feat(self, feat):\n",
    "        # feat.shape: (b, c, h, w)\n",
    "        b, c, h, w = feat.size()\n",
    "        # feat.shape: (b, c, h, w) -> (b, c, h*w)\n",
    "        feat = feat.view(b, c, h * w)\n",
    "        # feat.shape: (b, c, h*w) -> (b, h*w, c)\n",
    "        feat = feat.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        return feat\n",
    "\n",
    "    def gather_feat(self, feat, ind, mask=None):\n",
    "        # feat.shape: (b, h*w, c)\n",
    "        feat_b, hw, c = feat.size()\n",
    "        # ind.shape: (b, 100)\n",
    "        ind_b, n = ind.size()\n",
    "        assert feat_b == ind_b\n",
    "        b = ind_b\n",
    "        # ind.shape: (b, 100) -> (b, 100, 1) -> (b, 100, c)\n",
    "        ind = ind.unsqueeze(2).expand(b, n, c)\n",
    "        # feat.shape: (b, h*w, c) -> (b, 100, c)\n",
    "        feat = feat.gather(1, ind)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(2).expand_as(feat)\n",
    "            feat = feat[mask]\n",
    "            feat = feat.view(-1, c)\n",
    "\n",
    "        return feat\n",
    "\n",
    "    def nms(self, heatmap, kernel=3):\n",
    "        pad = (kernel - 1) // 2\n",
    "        hmax = torch.nn.functional.max_pool2d(\n",
    "            heatmap, (kernel, kernel), stride=1, padding=pad\n",
    "        )\n",
    "        keep = (hmax == heatmap).float()\n",
    "        heatmap = heatmap * keep\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "    def topk(self, heatmap, K=100):\n",
    "        # heatmap.shape: (b, c, h, w)\n",
    "        b, c, h, w = heatmap.size()\n",
    "        # heatmap.shape: (b, c, h, w) -> (b, c, h*w)\n",
    "        heatmap = heatmap.view(b, c, -1)\n",
    "        topk_scores, topk_inds = torch.topk(heatmap, K)\n",
    "        topk_inds = topk_inds % (h * w)\n",
    "        topk_ys = (topk_inds / w).int().float()\n",
    "        topk_xs = (topk_inds % w).int().float()\n",
    "        # (b, c, h*w) -> (b, c*h*w)\n",
    "        heatmap_flat = heatmap.view(b, -1)\n",
    "        topk_score, topk_ind = torch.topk(heatmap_flat, K)\n",
    "        topk_clses = (topk_ind / K).int()\n",
    "        topk_inds = self.gather_feat(topk_inds.view(b, -1, 1), topk_ind).view(b, K)\n",
    "        topk_ys = self.gather_feat(topk_ys.view(b, -1, 1), topk_ind).view(b, K)\n",
    "        topk_xs = self.gather_feat(topk_xs.view(b, -1, 1), topk_ind).view(b, K)\n",
    "\n",
    "        return topk_score, topk_inds, topk_clses, topk_ys, topk_xs\n",
    "\n",
    "    def decode(self, K=100):\n",
    "        b, c, h, w = self.hm.size()\n",
    "        hm = self.nms(self.hm)\n",
    "        scores, inds, clses, ys, xs = self.topk(hm)\n",
    "\n",
    "        reg = self.transpose_feat(self.reg)\n",
    "        reg = self.gather_feat(reg, inds)\n",
    "        reg = reg.view(b, K, 2)\n",
    "\n",
    "        xs = xs.view(b, K, 1) + reg[:, :, 0:1]\n",
    "        ys = ys.view(b, K, 1) + reg[:, :, 1:2]\n",
    "\n",
    "        ab = self.transpose_feat(self.ab)\n",
    "        ab = self.gather_feat(ab, inds)\n",
    "        ab = ab.view(b, K, 2)\n",
    "\n",
    "        ang = self.transpose_feat(self.ang)\n",
    "        ang = self.gather_feat(ang, inds)\n",
    "        ang = ang.view(b, K, 1)\n",
    "\n",
    "        clses = clses.view(b, K, 1).float()\n",
    "        scores = scores.view(b, K, 1)\n",
    "        bboxes = torch.cat([xs, ys, ab[..., 0:1], ab[..., 1:2], ang], dim=2)\n",
    "\n",
    "        detections = torch.cat([bboxes, scores, clses], dim=2)\n",
    "        print(bboxes)\n",
    "        # print(detections)\n",
    "\n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = PredDecoder(pred).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(det), det.device, det.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 设置环境变量\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# det.detach().cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EvEye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
