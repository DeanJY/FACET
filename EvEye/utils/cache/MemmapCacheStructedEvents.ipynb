{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "\n",
    "from EvEye.utils.processor.TxtProcessor import TxtProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_structed_arrays(arrays: list) -> np.ndarray:\n",
    "    total_length = sum([array.shape[0] for array in arrays])\n",
    "    merged_array = np.zeros(total_length, dtype=arrays[0].dtype)\n",
    "    current_index = 0\n",
    "    for array in tqdm(arrays, desc=\"Merging arrays...\"):\n",
    "        end_index = current_index + array.shape[0]\n",
    "        merged_array[current_index:end_index] = array\n",
    "        current_index = end_index\n",
    "    return merged_array\n",
    "\n",
    "\n",
    "def get_indices(arrays: list) -> np.ndarray:\n",
    "    indices_array = np.zeros((len(arrays), 2), dtype=np.int32)\n",
    "    current_index = 0\n",
    "    for index, array in tqdm(enumerate(arrays), desc=\"Getting indices...\"):\n",
    "        end_index = current_index + array.shape[0]\n",
    "        indices_array[index] = [current_index, end_index]\n",
    "        current_index = end_index\n",
    "    return indices_array\n",
    "\n",
    "\n",
    "def create_memmap(data, data_file, info_file):\n",
    "    mmap = np.memmap(data_file, dtype=data.dtype, mode=\"w+\", shape=data.shape)\n",
    "    mmap[:] = data\n",
    "    mmap.flush()\n",
    "    with open(info_file, \"w\") as f:\n",
    "        f.write(f\"Data shape: {data.shape}\\n\")\n",
    "        f.write(f\"Data dtype: {data.dtype}\\n\")\n",
    "    return mmap\n",
    "\n",
    "\n",
    "def load_memmap(data_file, info_file):\n",
    "    with open(info_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        shape_line = lines[0].strip()\n",
    "        dtype_line = lines[1].strip()\n",
    "\n",
    "        shape_str = shape_line.split(\": \")[1]\n",
    "        shape = tuple(\n",
    "            int(num) for num in shape_str.strip(\"()\").split(\",\") if num.strip()\n",
    "        )\n",
    "\n",
    "        dtype_str = dtype_line.split(\": \")[1]\n",
    "        dtype = eval(dtype_str)\n",
    "\n",
    "    mmap = np.memmap(data_file, dtype=dtype, mode=\"r\", shape=shape)\n",
    "    return mmap\n",
    "\n",
    "\n",
    "def cache_structed_events(batch_size, data_base_path, output_path, start_batch=0):\n",
    "    data_base_path = Path(data_base_path)\n",
    "    data_paths = natsorted(data_base_path.glob(\"*.txt\"))\n",
    "\n",
    "    output_path = Path(output_path)\n",
    "    output_data_path = output_path / \"cached_data\"\n",
    "    output_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    batch_counter = start_batch\n",
    "    batch_events = []\n",
    "\n",
    "    # 计算已经处理的文件数量\n",
    "    files_processed = start_batch * batch_size\n",
    "\n",
    "    for index, data_path in enumerate(\n",
    "        tqdm(data_paths[files_processed:], total=len(data_paths) - files_processed)\n",
    "    ):\n",
    "        event = TxtProcessor(data_path).load_events_from_txt()\n",
    "        batch_events.append(event)\n",
    "\n",
    "        if (index + 1) % batch_size == 0 or (index + 1) == len(\n",
    "            data_paths\n",
    "        ) - files_processed:\n",
    "            events_merged = merge_structed_arrays(batch_events)\n",
    "            events_indices = get_indices(batch_events)\n",
    "            create_memmap(\n",
    "                events_merged,\n",
    "                f\"{output_data_path}/events_batch_{batch_counter}.memmap\",\n",
    "                f\"{output_data_path}/events_batch_info_{batch_counter}.txt\",\n",
    "            )\n",
    "            np.save(\n",
    "                f\"{output_data_path}/events_indices_{batch_counter}.npy\", events_indices\n",
    "            )\n",
    "\n",
    "            batch_events = []\n",
    "            batch_counter += 1\n",
    "\n",
    "\n",
    "def cache_structed_ellipses(batch_size, ellipse_path, output_path):\n",
    "    ellipse_path = Path(ellipse_path)\n",
    "    ellipse_paths = natsorted(ellipse_path.glob(\"*.txt\"))\n",
    "\n",
    "    output_path = Path(output_path)\n",
    "    output_data_path = output_path / \"cached_ellipse\"\n",
    "    output_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    batch_conter = 0\n",
    "    batch_ellipses = []\n",
    "\n",
    "    for index, ellipse_path in enumerate(tqdm(ellipse_paths, total=len(ellipse_paths))):\n",
    "        ellipse = TxtProcessor(ellipse_path).load_ellipses_from_txt()\n",
    "        batch_ellipses.append(ellipse)\n",
    "\n",
    "        if (index + 1) % batch_size == 0 or (index + 1) == len(ellipse_paths):\n",
    "            ellipses_merged = merge_structed_arrays(batch_ellipses)\n",
    "            ellipses_indices = get_indices(batch_ellipses)\n",
    "            create_memmap(\n",
    "                ellipses_merged,\n",
    "                f\"{output_data_path}/ellipses_batch_{batch_conter}.memmap\",\n",
    "                f\"{output_data_path}/ellipses_batch_info_{batch_conter}.txt\",\n",
    "            )\n",
    "            np.save(\n",
    "                f\"{output_data_path}/ellipses_indices_{batch_conter}.npy\",\n",
    "                ellipses_indices,\n",
    "            )\n",
    "            batch_ellipses = []\n",
    "            batch_conter += 1\n",
    "\n",
    "\n",
    "def cache_structed_data(\n",
    "    time_window, frames_per_segment, batch_size, base_path, output_path\n",
    "):\n",
    "    base_path = Path(base_path)\n",
    "    output_path = Path(output_path)\n",
    "    output_data_path = output_path / \"cached_data\"\n",
    "    output_label_path = output_path / \"cached_label\"\n",
    "    output_data_path.mkdir(parents=True, exist_ok=True)\n",
    "    output_label_path.mkdir(parents=True, exist_ok=True)\n",
    "    data_base_path = base_path / \"data\"\n",
    "    label_base_path = base_path / \"label\"\n",
    "    data_paths = natsorted(data_base_path.glob(\"*.txt\"))\n",
    "    label_paths = natsorted(label_base_path.glob(\"*.txt\"))\n",
    "\n",
    "    batch_counter = 0\n",
    "    batch_events = []\n",
    "    batch_labels = []\n",
    "    num_frames_list = []\n",
    "    num_segments_list = []\n",
    "    for index, (data_path, label_path) in enumerate(\n",
    "        tqdm(zip(data_paths, label_paths), total=len(data_paths))\n",
    "    ):\n",
    "        event = TxtProcessor(data_path).load_events_from_txt()\n",
    "        label = TxtProcessor(label_path).load_labels_from_txt()\n",
    "\n",
    "        index_label = 0\n",
    "        while index_label < len(label['t']):\n",
    "            start_time_first = max(event['t'][0], label['t'][index_label] - time_window)\n",
    "            end_time_first = label['t'][index_label]\n",
    "            if start_time_first >= end_time_first:\n",
    "                index_label += 1\n",
    "                if index_label < len(label['t']):\n",
    "                    continue\n",
    "                else:\n",
    "                    raise ValueError(\"No event before the first label\")\n",
    "            else:\n",
    "                break\n",
    "        label = label[index_label:]\n",
    "        num_frames = label.shape[0]\n",
    "        num_frames_list.append(num_frames)\n",
    "        num_segments_list.append(num_frames // frames_per_segment)\n",
    "\n",
    "        batch_events.append(event)\n",
    "        batch_labels.append(label)\n",
    "\n",
    "        if (index + 1) % batch_size == 0 or (index + 1) == len(data_paths):\n",
    "            events_merged = merge_structed_arrays(batch_events)\n",
    "            events_indices = get_indices(batch_events)\n",
    "            labels_merged = merge_structed_arrays(batch_labels)\n",
    "            labels_indices = get_indices(batch_labels)\n",
    "            create_memmap(\n",
    "                events_merged,\n",
    "                f\"{output_data_path}/events_batch_{batch_counter}.memmap\",\n",
    "                f\"{output_data_path}/events_batch_info_{batch_counter}.txt\",\n",
    "            )\n",
    "            create_memmap(\n",
    "                labels_merged,\n",
    "                f\"{output_label_path}/labels_batch_{batch_counter}.memmap\",\n",
    "                f\"{output_label_path}/labels_batch_info_{batch_counter}.txt\",\n",
    "            )\n",
    "            np.save(\n",
    "                f\"{output_data_path}/events_indices_{batch_counter}.npy\", events_indices\n",
    "            )\n",
    "            np.save(\n",
    "                f\"{output_label_path}/labels_indices_{batch_counter}.npy\",\n",
    "                labels_indices,\n",
    "            )\n",
    "\n",
    "            batch_counter += 1\n",
    "\n",
    "\n",
    "def load_cached_structed_events(events_path):\n",
    "    events_list = []\n",
    "    events_path = Path(events_path)\n",
    "    events_paths = natsorted(events_path.glob(\"events_batch_*.memmap\"))\n",
    "    events_info_paths = natsorted(events_path.glob(\"events_batch_info_*.txt\"))\n",
    "    events_indices_paths = natsorted(events_path.glob(\"events_indices_*.npy\"))\n",
    "\n",
    "    for events_path, events_info_path, events_indices_path in zip(\n",
    "        events_paths, events_info_paths, events_indices_paths\n",
    "    ):\n",
    "        events = load_memmap(events_path, events_info_path)\n",
    "        events_indices = np.load(events_indices_path)\n",
    "\n",
    "        for index in range(events_indices.shape[0]):\n",
    "            start_index, end_index = events_indices[index]\n",
    "            events_list.append(events[start_index:end_index])\n",
    "\n",
    "    return events_list\n",
    "\n",
    "\n",
    "def load_cached_structed_labels(labels_path):\n",
    "    labels_list = []\n",
    "    labels_path = Path(labels_path)\n",
    "    labels_paths = natsorted(labels_path.glob(\"labels_batch_*.memmap\"))\n",
    "    labels_info_paths = natsorted(labels_path.glob(\"labels_batch_info_*.txt\"))\n",
    "    labels_indices_paths = natsorted(labels_path.glob(\"labels_indices_*.npy\"))\n",
    "\n",
    "    for labels_path, labels_info_path, labels_indices_path in zip(\n",
    "        labels_paths, labels_info_paths, labels_indices_paths\n",
    "    ):\n",
    "        labels = load_memmap(labels_path, labels_info_path)\n",
    "        labels_indices = np.load(labels_indices_path)\n",
    "\n",
    "        for index in range(labels_indices.shape[0]):\n",
    "            start_index, end_index = labels_indices[index]\n",
    "            labels_list.append(labels[start_index:end_index])\n",
    "\n",
    "    return labels_list\n",
    "\n",
    "\n",
    "def load_cached_structed_ellipses(ellipses_path):\n",
    "    ellipses_list = []\n",
    "    ellipses_path = Path(ellipses_path)\n",
    "    ellipses_paths = natsorted(ellipses_path.glob(\"ellipses_batch_*.memmap\"))\n",
    "    ellipses_info_paths = natsorted(ellipses_path.glob(\"ellipses_batch_info_*.txt\"))\n",
    "    ellipses_indices_paths = natsorted(ellipses_path.glob(\"ellipses_indices_*.npy\"))\n",
    "\n",
    "    for ellipses_path, ellipses_info_path, ellipses_indices_path in zip(\n",
    "        ellipses_paths, ellipses_info_paths, ellipses_indices_paths\n",
    "    ):\n",
    "        ellipses = load_memmap(ellipses_path, ellipses_info_path)\n",
    "        ellipses_indices = np.load(ellipses_indices_path)\n",
    "\n",
    "        for index in range(ellipses_indices.shape[0]):\n",
    "            start_index, end_index = ellipses_indices[index]\n",
    "            ellipses_list.append(ellipses[start_index:end_index])\n",
    "\n",
    "    return ellipses_list\n",
    "\n",
    "\n",
    "def get_nums(labels_path, frames_per_segment=50):\n",
    "    labels_path = Path(labels_path)\n",
    "    labels_indices_paths = natsorted(labels_path.glob(\"labels_indices_*.npy\"))\n",
    "\n",
    "    num_frames_list = []\n",
    "    num_segments_list = []\n",
    "    for labels_indices_path in labels_indices_paths:\n",
    "        labels_indices = np.load(labels_indices_path)\n",
    "        for index in range(labels_indices.shape[0]):\n",
    "            start_index, end_index = labels_indices[index]\n",
    "            num_frames = end_index - start_index\n",
    "            num_frames_list.append(num_frames)\n",
    "            num_segments_list.append(num_frames // frames_per_segment)\n",
    "    total_segments = sum(num_segments_list)\n",
    "    return num_frames_list, num_segments_list, total_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/362 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging arrays...: 100%|██████████| 362/362 [00:00<00:00, 16911.95it/s]\n",
      "Getting indices...: 362it [00:00, 338235.25it/s]\n",
      "100%|██████████| 362/362 [00:00<00:00, 1348.52it/s]\n"
     ]
    }
   ],
   "source": [
    "cache_structed_ellipses(\n",
    "    500,\n",
    "    \"/mnt/data2T/junyuan/eye-tracking/datasets/DavisEyeEllipseLabelsV2\",\n",
    "    \"/mnt/data2T/junyuan/eye-tracking/datasets/MemmapDavisEyeEllipseDataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ellipses_path = \"/mnt/data2T/junyuan/eye-tracking/datasets/MemmapDavisEyeEllipseDataset/cached_ellipse\"\n",
    "ellipses_list = load_cached_structed_ellipses(ellipses_path)\n",
    "len(ellipses_list[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_path = (\n",
    "    \"/mnt/data2T/junyuan/eye-tracking/datasets/MemmapDavisEyeEllipseDataset/cached_data\"\n",
    ")\n",
    "events = load_cached_structed_events(events_path)\n",
    "len(events)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EvEye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
